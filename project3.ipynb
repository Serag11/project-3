{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as fn\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from skimage import io\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############  HYPERPARAMETERS  #############\n",
    "test_size = 0.1\n",
    "valid_size = 0.2\n",
    "keep_prob = 0.2\n",
    "nb_epoch = 10\n",
    "samples_per_epoch = 20000\n",
    "save_best_only = True\n",
    "learning_rate = 1.0e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_shape = (3, 160, 320)\n",
    "keep_prob = 0.5  # Dropout keep probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############  DATA Getting  #############\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        # img_path = self.img_labels.iloc[idx, 0]\n",
    "        image = io.imread(img_path)\n",
    "        label1 = torch.tensor(float(self.img_labels.iloc[idx , 3]))\n",
    "        label2 = torch.tensor(float((self.img_labels.iloc[idx , 4])))\n",
    "        label1 = label1.numpy()\n",
    "        label2 = label2.numpy()\n",
    "        label = np.column_stack([label1,label2])\n",
    "        label = torch.from_numpy(label)\n",
    "        label=label.flatten(0,-1)        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sg/Desktop/project/project3.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sg/Desktop/project/project3.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     cutted_dataset\u001b[39m.\u001b[39mappend((ig , l))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sg/Desktop/project/project3.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m# imshow(ig)    \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sg/Desktop/project/project3.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m train_dataset, validation_dataset \u001b[39m=\u001b[39m train_test_split(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sg/Desktop/project/project3.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     dataset, test_size\u001b[39m=\u001b[39;49mtest_size, random_state\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sg/Desktop/project/project3.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# train_dataset, validation_dataset = train_test_split(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sg/Desktop/project/project3.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m#     trainvalid_dataset, test_size=valid_size, random_state=0)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sg/Desktop/project/project3.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m train_data_loader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sg/Desktop/project/project3.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     train_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2672\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2668\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m   2670\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39marrays[\u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mstratify))\n\u001b[0;32m-> 2672\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\n\u001b[1;32m   2673\u001b[0m     chain\u001b[39m.\u001b[39;49mfrom_iterable(\n\u001b[1;32m   2674\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m arrays\n\u001b[1;32m   2675\u001b[0m     )\n\u001b[1;32m   2676\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2674\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2668\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m   2670\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39marrays[\u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mstratify))\n\u001b[1;32m   2672\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   2673\u001b[0m     chain\u001b[39m.\u001b[39mfrom_iterable(\n\u001b[0;32m-> 2674\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays\n\u001b[1;32m   2675\u001b[0m     )\n\u001b[1;32m   2676\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/sklearn/utils/__init__.py:357\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m _array_indexing(X, indices, indices_dtype, axis\u001b[39m=\u001b[39maxis)\n\u001b[1;32m    356\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m     \u001b[39mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/sklearn/utils/__init__.py:211\u001b[0m, in \u001b[0;36m_list_indexing\u001b[0;34m(X, key, key_dtype)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(compress(X, key))\n\u001b[1;32m    210\u001b[0m \u001b[39m# key is a integer array-like of key\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m [X[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m key]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/sklearn/utils/__init__.py:211\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(compress(X, key))\n\u001b[1;32m    210\u001b[0m \u001b[39m# key is a integer array-like of key\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m [X[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m key]\n",
      "\u001b[1;32m/home/sg/Desktop/project/project3.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sg/Desktop/project/project3.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m img_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_labels\u001b[39m.\u001b[39miloc[idx, \u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sg/Desktop/project/project3.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# img_path = self.img_labels.iloc[idx, 0]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sg/Desktop/project/project3.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m image \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mimread(img_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sg/Desktop/project/project3.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m label1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mfloat\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_labels\u001b[39m.\u001b[39miloc[idx , \u001b[39m3\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sg/Desktop/project/project3.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m label2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mfloat\u001b[39m((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_labels\u001b[39m.\u001b[39miloc[idx , \u001b[39m4\u001b[39m])))\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/skimage/io/_io.py:53\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     50\u001b[0m         plugin \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtifffile\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[39mwith\u001b[39;00m file_or_url_context(fname) \u001b[39mas\u001b[39;00m fname:\n\u001b[0;32m---> 53\u001b[0m     img \u001b[39m=\u001b[39m call_plugin(\u001b[39m'\u001b[39;49m\u001b[39mimread\u001b[39;49m\u001b[39m'\u001b[39;49m, fname, plugin\u001b[39m=\u001b[39;49mplugin, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mplugin_args)\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(img, \u001b[39m'\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/skimage/io/manage_plugins.py:205\u001b[0m, in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCould not find the plugin \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mplugin\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m for \u001b[39m\u001b[39m{\u001b[39;00mkind\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 205\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/skimage/io/_plugins/imageio_plugin.py:11\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m@wraps\u001b[39m(imageio_imread)\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimread\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 11\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(imageio_imread(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m out\u001b[39m.\u001b[39mflags[\u001b[39m'\u001b[39m\u001b[39mWRITEABLE\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     13\u001b[0m         out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/imageio/v3.py:54\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(uri, index, plugin, extension, format_hint, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m     call_kwargs[\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m index\n\u001b[1;32m     53\u001b[0m \u001b[39mwith\u001b[39;00m imopen(uri, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mplugin_kwargs) \u001b[39mas\u001b[39;00m img_file:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(img_file\u001b[39m.\u001b[39;49mread(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcall_kwargs))\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/imageio/plugins/pillow.py:231\u001b[0m, in \u001b[0;36mPillowPlugin.read\u001b[0;34m(self, index, mode, rotate, apply_gamma, writeable_output, pilmode, exifrotate, as_gray)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(index, \u001b[39mint\u001b[39m):\n\u001b[1;32m    229\u001b[0m     \u001b[39m# will raise IO error if index >= number of frames in image\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_image\u001b[39m.\u001b[39mseek(index)\n\u001b[0;32m--> 231\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_transforms(\n\u001b[1;32m    232\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_image, mode, rotate, apply_gamma, writeable_output\n\u001b[1;32m    233\u001b[0m     )\n\u001b[1;32m    234\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter(\n\u001b[1;32m    236\u001b[0m         mode\u001b[39m=\u001b[39mmode,\n\u001b[1;32m    237\u001b[0m         rotate\u001b[39m=\u001b[39mrotate,\n\u001b[1;32m    238\u001b[0m         apply_gamma\u001b[39m=\u001b[39mapply_gamma,\n\u001b[1;32m    239\u001b[0m         writeable_output\u001b[39m=\u001b[39mwriteable_output,\n\u001b[1;32m    240\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/imageio/plugins/pillow.py:314\u001b[0m, in \u001b[0;36mPillowPlugin._apply_transforms\u001b[0;34m(self, image, mode, rotate, apply_gamma, writeable_output)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[39melse\u001b[39;00m:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    311\u001b[0m         \u001b[39m# Let pillow know that it is okay to return 16-bit\u001b[39;00m\n\u001b[1;32m    312\u001b[0m         image\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m desired_mode\n\u001b[0;32m--> 314\u001b[0m image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(image)\n\u001b[1;32m    316\u001b[0m meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata(index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_image\u001b[39m.\u001b[39mtell(), exclude_applied\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m rotate \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mOrientation\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m meta:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/PIL/Image.py:701\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m         new[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtobytes(\u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    700\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 701\u001b[0m         new[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtobytes()\n\u001b[1;32m    702\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(e, (\u001b[39mMemoryError\u001b[39;00m, \u001b[39mRecursionError\u001b[39;00m)):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/PIL/Image.py:758\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[39mif\u001b[39;00m encoder_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m args \u001b[39m==\u001b[39m ():\n\u001b[1;32m    756\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\n\u001b[0;32m--> 758\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    760\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheight \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    761\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = CustomImageDataset(annotations_file= r\"/home/sg/Downloads/simulator-linux/data/driving_log.csv\",\n",
    "                             img_dir=r\"/home/sg/Downloads/simulator-linux/data/IMG\",\n",
    "                             transform= transforms.ToTensor() )\n",
    "cutted_dataset = []\n",
    "\n",
    "def imshow(imgs):\n",
    "    imgs = imgs / 2 + 0.5   # unnormalize\n",
    "    npimgs = imgs.numpy()\n",
    "    plt.imshow(np.transpose(npimgs, (1, 2, 0)))\n",
    "    plt.show()\n",
    "transpil = transforms.ToPILImage()\n",
    "transtensor = transforms.ToTensor()\n",
    "\n",
    "\n",
    "\n",
    "for i , (ig , l) in enumerate(dataset):\n",
    "    \n",
    "    # img_grid = torchvision.utils.make_grid(ig[0:25], nrow=5)\n",
    "    # imshow(img_grid)    \n",
    "    ig = fn.crop(ig, 60 , 0 ,100, 320)\n",
    "    # imshow(ig)    \n",
    "    # imshow(ig.resize_(3,160,320))    \n",
    "    ig = transpil(ig)\n",
    "    resize = fn.resize(ig, size=[160,320])\n",
    "    ig = transtensor(resize)\n",
    "    cutted_dataset.append((ig , l))\n",
    "    # imshow(ig)    \n",
    "\n",
    "train_dataset, validation_dataset = train_test_split(\n",
    "    dataset, test_size=test_size, random_state=0)\n",
    "# train_dataset, validation_dataset = train_test_split(\n",
    "#     trainvalid_dataset, test_size=valid_size, random_state=0)\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset, batch_size=30, shuffle=True)\n",
    "validation_data_loader = DataLoader(\n",
    "    validation_dataset, batch_size=30, shuffle=True)\n",
    "# testing_data_loader = DataLoader(\n",
    "#     testing_dataset, batch_size=30, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Architicture\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.keep_prob = keep_prob\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 24, kernel_size=5, stride=2),      # 34*34\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(24, 36, kernel_size=5, stride=2),        # 16*16\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(36, 48, kernel_size=5, stride=2),      # 7*7\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(48, 64, kernel_size=3),             # 5*5\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3),       # 3*3\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Dropout(keep_prob),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(27456 , 100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(10, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x / 127.5 - 1.0     # Normalize the input\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    # def get_flatten_size(self):\n",
    "    #     with torch.no_grad():\n",
    "    #         x = torch.zeros(1, *self.input_shape)\n",
    "    #         x = self.conv_layers(x)\n",
    "    #         return x.view(1, -1).size(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################  TRAINING  #################\n",
    "def train_model(model):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    best_loss = 100.0\n",
    "    for epoch in range(nb_epoch):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_data_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in validation_data_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                valid_loss += loss.item()\n",
    "        valid_loss /= len(validation_data_loader)\n",
    "        print(f'Epoch {epoch+1}/{nb_epoch} - Training Loss: {epoch_loss:.4f} - Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "        # Save the best model\n",
    "        if save_best_only and valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model, \"model.pth\")\n",
    "            \n",
    "    print(f'Training completed with best loss = {best_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**# MAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training Loss: 0.1744 - Validation Loss: 0.1293\n",
      "Epoch 2/10 - Training Loss: 0.1498 - Validation Loss: 0.1223\n",
      "Epoch 3/10 - Training Loss: 0.1484 - Validation Loss: 0.1226\n",
      "Epoch 4/10 - Training Loss: 0.1485 - Validation Loss: 0.1279\n",
      "Epoch 5/10 - Training Loss: 0.1483 - Validation Loss: 0.1223\n",
      "Epoch 6/10 - Training Loss: 0.1492 - Validation Loss: 0.1318\n",
      "Epoch 7/10 - Training Loss: 0.1482 - Validation Loss: 0.1309\n",
      "Epoch 8/10 - Training Loss: 0.1486 - Validation Loss: 0.1281\n",
      "Epoch 9/10 - Training Loss: 0.1483 - Validation Loss: 0.1279\n",
      "Epoch 10/10 - Training Loss: 0.1479 - Validation Loss: 0.1296\n",
      "Training completed with best loss = 0.1223\n"
     ]
    }
   ],
   "source": [
    "############  MAIN  ###############\n",
    "\n",
    "model = MyModel()\n",
    "train_model(model)\n",
    "# def imshow(imgs):\n",
    "#     imgs = imgs / 2 + 0.5   # unnormalize\n",
    "#     npimgs = imgs.numpy()\n",
    "#     plt.imshow(np.transpose(npimgs, (1, 2, 0)))\n",
    "#     plt.show()\n",
    "\n",
    "# # one batch of random training images\n",
    "# for i , (ig , l) in enumerate(train_data_loader):\n",
    "#     if i == 1:\n",
    "#         img_grid = torchvision.utils.make_grid(ig[0:25], nrow=5)\n",
    "#         imshow(img_grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
