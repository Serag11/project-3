{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as fn\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from skimage import io\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############  HYPERPARAMETERS  #############\n",
    "test_size = 0.1\n",
    "valid_size = 0.2\n",
    "keep_prob = 0.2\n",
    "nb_epoch = 10\n",
    "samples_per_epoch = 20000\n",
    "save_best_only = True\n",
    "learning_rate = 1.0e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_shape = (3, 160, 320)\n",
    "keep_prob = 0.5  # Dropout keep probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############  DATA Getting  #############\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        # img_path = self.img_labels.iloc[idx, 0]\n",
    "        image = io.imread(img_path)\n",
    "        label1 = torch.tensor(float(self.img_labels.iloc[idx , 3]))\n",
    "        label2 = torch.tensor(float((self.img_labels.iloc[idx , 4])))\n",
    "        label1 = label1.numpy()\n",
    "        label2 = label2.numpy()\n",
    "        label = np.column_stack([label1,label2])\n",
    "        label = torch.from_numpy(label)\n",
    "        label=label.flatten(0,-1)        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomImageDataset(annotations_file= r\"/home/sg/Downloads/simulator-linux/data/driving_log.csv\",\n",
    "                             img_dir=r\"/home/sg/Downloads/simulator-linux/data/IMG\",\n",
    "                             transform= transforms.ToTensor() )\n",
    "cutted_dataset = []\n",
    "\n",
    "def imshow(imgs):\n",
    "    imgs = imgs / 2 + 0.5   # unnormalize\n",
    "    npimgs = imgs.numpy()\n",
    "    plt.imshow(np.transpose(npimgs, (1, 2, 0)))\n",
    "    plt.show()\n",
    "transpil = transforms.ToPILImage()\n",
    "transtensor = transforms.ToTensor()\n",
    "\n",
    "\n",
    "\n",
    "for i , (ig , l) in enumerate(dataset):\n",
    "    \n",
    "    # img_grid = torchvision.utils.make_grid(ig[0:25], nrow=5)\n",
    "    # imshow(img_grid)    \n",
    "    ig = fn.crop(ig, 60 , 0 ,100, 320)\n",
    "    # imshow(ig)    \n",
    "    # imshow(ig.resize_(3,160,320))    \n",
    "    ig = transpil(ig)\n",
    "    resize = fn.resize(ig, size=[160,320])\n",
    "    ig = transtensor(resize)\n",
    "    cutted_dataset.append((ig , l))\n",
    "    # imshow(ig)    \n",
    "\n",
    "train_dataset, validation_dataset = train_test_split(\n",
    "    dataset, test_size=test_size, random_state=0)\n",
    "# train_dataset, validation_dataset = train_test_split(\n",
    "#     trainvalid_dataset, test_size=valid_size, random_state=0)\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset, batch_size=30, shuffle=True)\n",
    "validation_data_loader = DataLoader(\n",
    "    validation_dataset, batch_size=30, shuffle=True)\n",
    "# testing_data_loader = DataLoader(\n",
    "#     testing_dataset, batch_size=30, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Architicture\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.keep_prob = keep_prob\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 24, kernel_size=5, stride=2),      # 34*34\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(24, 36, kernel_size=5, stride=2),        # 16*16\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(36, 48, kernel_size=5, stride=2),      # 7*7\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(48, 64, kernel_size=3),             # 5*5\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3),       # 3*3\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Dropout(keep_prob),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(27456 , 100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(10, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x / 127.5 - 1.0     # Normalize the input\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    # def get_flatten_size(self):\n",
    "    #     with torch.no_grad():\n",
    "    #         x = torch.zeros(1, *self.input_shape)\n",
    "    #         x = self.conv_layers(x)\n",
    "    #         return x.view(1, -1).size(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################  TRAINING  #################\n",
    "def train_model(model):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    best_loss = 100.0\n",
    "    for epoch in range(nb_epoch):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_data_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in validation_data_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                valid_loss += loss.item()\n",
    "        valid_loss /= len(validation_data_loader)\n",
    "        print(f'Epoch {epoch+1}/{nb_epoch} - Training Loss: {epoch_loss:.4f} - Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "        # Save the best model\n",
    "        if save_best_only and valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model, \"model.pth\")\n",
    "            \n",
    "    print(f'Training completed with best loss = {best_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**# MAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training Loss: 0.1820 - Validation Loss: 0.1272\n",
      "Epoch 2/10 - Training Loss: 0.1508 - Validation Loss: 0.1326\n",
      "Epoch 3/10 - Training Loss: 0.1507 - Validation Loss: 0.1317\n",
      "Epoch 4/10 - Training Loss: 0.1523 - Validation Loss: 0.1275\n",
      "Epoch 5/10 - Training Loss: 0.1486 - Validation Loss: 0.1246\n",
      "Epoch 6/10 - Training Loss: 0.1492 - Validation Loss: 0.1324\n",
      "Epoch 7/10 - Training Loss: 0.1504 - Validation Loss: 0.1370\n",
      "Epoch 8/10 - Training Loss: 0.1494 - Validation Loss: 0.1273\n",
      "Epoch 9/10 - Training Loss: 0.1490 - Validation Loss: 0.1298\n",
      "Epoch 10/10 - Training Loss: 0.1485 - Validation Loss: 0.1318\n",
      "Training completed with best loss = 0.1246\n"
     ]
    }
   ],
   "source": [
    "############  MAIN  ###############\n",
    "\n",
    "model = MyModel()\n",
    "train_model(model)\n",
    "# def imshow(imgs):\n",
    "#     imgs = imgs / 2 + 0.5   # unnormalize\n",
    "#     npimgs = imgs.numpy()\n",
    "#     plt.imshow(np.transpose(npimgs, (1, 2, 0)))\n",
    "#     plt.show()\n",
    "\n",
    "# # one batch of random training images\n",
    "# for i , (ig , l) in enumerate(train_data_loader):\n",
    "#     if i == 1:\n",
    "#         img_grid = torchvision.utils.make_grid(ig[0:25], nrow=5)\n",
    "#         imshow(img_grid)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
